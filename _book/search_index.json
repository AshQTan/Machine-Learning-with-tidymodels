[
["decision-trees.html", "Chapter 5 Decision Trees 5.1 Load packages 5.2 Load data 5.3 Overview 5.4 Non-tidy 5.5 Tidy models", " Chapter 5 Decision Trees 5.1 Load packages library(rpart) library(rpart.plot) library(rio) # painless data import and export library(tidyverse) # tidyverse packages ## ── Attaching packages ─── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.3 ✓ dplyr 1.0.2 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0 ## ── Conflicts ────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(tidymodels) # tidymodels framework ## ── Attaching packages ── tidymodels 0.1.1 ── ## ✓ broom 0.7.0 ✓ recipes 0.1.13 ## ✓ dials 0.0.9 ✓ rsample 0.0.7 ## ✓ infer 0.5.3 ✓ tune 0.1.1 ## ✓ modeldata 0.0.2 ✓ workflows 0.2.0 ## ✓ parsnip 0.1.3 ✓ yardstick 0.0.7 ## ── Conflicts ───── tidymodels_conflicts() ── ## x scales::discard() masks purrr::discard() ## x dplyr::filter() masks stats::filter() ## x recipes::fixed() masks stringr::fixed() ## x dplyr::lag() masks stats::lag() ## x dials::prune() masks rpart::prune() ## x yardstick::spec() masks readr::spec() ## x recipes::step() masks stats::step() library(here) # reproducible way to find files ## here() starts at /home/jae/Machine-Learning-in-R library(glue) # glue strings and objects ## ## Attaching package: &#39;glue&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse theme_set(theme_minimal()) 5.2 Load data Load train_x_class, train_y_class, test_x_class, and test_y_class variables we defined in 02-preprocessing.Rmd for this classification task. # Objects: task_reg, task_class load(here(&quot;data&quot;, &quot;preprocessed.RData&quot;)) 5.3 Overview Decision trees are recursive partitioning methods that divide the predictor spaces into simpler regions and can be visualized in a tree-like structure. They attempt to classify data by dividing it into subsets according to a Y output variable and based on some predictors. Let’s see how a decision tree classifies if a person suffers from heart disease (target = 1) or not (target = 0). 5.4 Non-tidy 5.4.1 Fit a model set.seed(3) tree &lt;- rpart::rpart(train_y_class ~ ., data = train_x_class, # Use method = &quot;anova&quot; for a continuous outcome. method = &quot;class&quot;, # Can use &quot;gini&quot; for gini coefficient. parms = list(split = &quot;information&quot;)) # https://stackoverflow.com/questions/4553947/decision-tree-on-information-gain 5.4.2 Investigate Here is the text-based display of the decision tree. Yikes! :^( print(tree) ## n= 213 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 213 97 1 (0.45539906 0.54460094) ## 2) thal_X2&lt; 0.5 100 25 0 (0.75000000 0.25000000) ## 4) exang&gt;=0.5 47 3 0 (0.93617021 0.06382979) * ## 5) exang&lt; 0.5 53 22 0 (0.58490566 0.41509434) ## 10) thalach&lt; 130 7 0 0 (1.00000000 0.00000000) * ## 11) thalach&gt;=130 46 22 0 (0.52173913 0.47826087) ## 22) chol&gt;=244 19 5 0 (0.73684211 0.26315789) * ## 23) chol&lt; 244 27 10 1 (0.37037037 0.62962963) ## 46) chol&lt; 230 20 9 1 (0.45000000 0.55000000) ## 92) oldpeak&gt;=0.7 8 2 0 (0.75000000 0.25000000) * ## 93) oldpeak&lt; 0.7 12 3 1 (0.25000000 0.75000000) * ## 47) chol&gt;=230 7 1 1 (0.14285714 0.85714286) * ## 3) thal_X2&gt;=0.5 113 22 1 (0.19469027 0.80530973) ## 6) thalach&lt; 162.5 62 19 1 (0.30645161 0.69354839) ## 12) sex_X1&gt;=0.5 29 14 0 (0.51724138 0.48275862) ## 24) thalach&lt; 131.5 8 1 0 (0.87500000 0.12500000) * ## 25) thalach&gt;=131.5 21 8 1 (0.38095238 0.61904762) ## 50) thalach&gt;=156.5 7 2 0 (0.71428571 0.28571429) * ## 51) thalach&lt; 156.5 14 3 1 (0.21428571 0.78571429) * ## 13) sex_X1&lt; 0.5 33 4 1 (0.12121212 0.87878788) * ## 7) thalach&gt;=162.5 51 3 1 (0.05882353 0.94117647) * Although interpreting the text can be intimidating, a decision tree’s main strength is its tree-like plot, which is much easier to interpret. rpart.plot::rpart.plot(tree) We can also look inside of tree to see what we can unpack. “variable.importance” is one we should check out! names(tree) ## [1] &quot;frame&quot; &quot;where&quot; &quot;call&quot; ## [4] &quot;terms&quot; &quot;cptable&quot; &quot;method&quot; ## [7] &quot;parms&quot; &quot;control&quot; &quot;functions&quot; ## [10] &quot;numresp&quot; &quot;splits&quot; &quot;variable.importance&quot; ## [13] &quot;y&quot; &quot;ordered&quot; tree$variable.importance ## thal_X2 thalach thal_X3 exang oldpeak slope_X2 chol ## 34.8546153 33.5429632 29.6264230 20.1781088 19.8238634 12.8908263 9.4650464 ## sex_X1 age ca_X2 trestbps cp_X2 slope_X1 ca_X3 ## 6.4228665 3.4926165 2.1173837 1.9228895 0.9689174 0.8355620 0.5555638 5.5 Tidy models 5.5.1 parsnip Build a model Specify a model Specify an engine Specify a mode tree_spec &lt;- decision_tree( # Mode mode = &quot;classification&quot;, # Tuning parameters; Not optimized yet cost_complexity = NULL) %&gt;% set_engine(&quot;rpart&quot;) # rpart, c5.0, spark Fit a model tree_fit &lt;- tree_spec %&gt;% fit_xy(x = train_x_class, y= train_y_class) 5.5.2 yardstick Let’s formally test prediction performance. Metrics accuracy: The proportion of the data predicted correctly precision: Positive predictive value recall (specificity): True positive rate (e.g., healthy people really healthy) From wikipedia To learn more about other metrics, check out the yardstick package references. # Define performance metrics metrics &lt;- yardstick::metric_set(accuracy, precision, recall) # Build an evaluation function evaluate_class &lt;- function(model){ # Bind ground truth and predicted values bind_cols(tibble(truth = test_y_class), # Ground truth predict(model, test_x_class)) %&gt;% # Predicted values # Calculate root mean-squared error metrics(truth = truth, estimate = .pred_class) } visualize_metrics &lt;- function(model){ evaluate_class(model) %&gt;% ggplot(aes(x = fct_reorder(.metric, .estimate), y = .estimate)) + geom_point() + labs(x = &quot;Metrics&quot;, y = &quot;Estimate&quot;)} visualize_metrics(tree_fit) - Visualize the confusion matrix. The following visualization code draws on Diego Usai’s medium post. visualize_conf_mat &lt;- function(model){ bind_cols(tibble(truth = test_y_class), # Ground truth predict(model, test_x_class)) %&gt;% conf_mat(truth, .pred_class) %&gt;% pluck(1) %&gt;% # Select index as_tibble() %&gt;% # Vector -&gt; data.frame ggplot(aes(Prediction, Truth, alpha = n)) + geom_tile(show.legend = FALSE) + geom_text(aes(label = n), color = &quot;white&quot;, alpha = 1, size = 8) } visualize_conf_mat(tree_fit) 5.5.3 tune In decision trees the main hyperparameter (configuration setting) is the complexity parameter (CP), but the name is a little counterintuitive; a high CP results in a simple decision tree with few splits, whereas a low CP results in a larger decision tree with many splits. rpart uses cross-validation internally to estimate the accuracy at various CP settings. We can review those to see what setting seems best. Print the results for various CP settings - we want the one with the lowest “xerror”. We can also plot the performance estimates for different CP settings. Trees of similar sizes might appear to be tied for lowest “xerror”, but a tree with fewer splits might be easier to interpret. # Show estimated error rate at different complexity parameter settings. printcp(tree) ## ## Classification tree: ## rpart::rpart(formula = train_y_class ~ ., data = train_x_class, ## method = &quot;class&quot;, parms = list(split = &quot;information&quot;)) ## ## Variables actually used in tree construction: ## [1] chol exang oldpeak sex_X1 thal_X2 thalach ## ## Root node error: 97/213 = 0.4554 ## ## n= 213 ## ## CP nsplit rel error xerror xstd ## 1 0.515464 0 1.00000 1.00000 0.074930 ## 2 0.024055 1 0.48454 0.48454 0.062394 ## 3 0.020619 4 0.41237 0.54639 0.065048 ## 4 0.010000 10 0.27835 0.52577 0.064207 # Plot those estimated error rates. plotcp(tree) Print detailed results, variable importance, and summary of splits. You can also get more fine-grained control by checking out the “control” argument inside the rpart function. Type ?rpart to learn more. Be sure to check out gormanalysis excellent overview to help internalize what you learned in this example. "]
]
