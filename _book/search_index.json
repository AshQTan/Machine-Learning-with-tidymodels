[
["decision-trees.html", "Chapter 5 Decision Trees 5.1 Load packages 5.2 Load data 5.3 Overview 5.4 Non-tidy 5.5 Tidy models", " Chapter 5 Decision Trees 5.1 Load packages library(rpart) library(rpart.plot) library(rio) # painless data import and export library(tidyverse) # tidyverse packages ## ── Attaching packages ──────────────────── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.3 ✓ dplyr 1.0.2 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0 ## ── Conflicts ──── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(tidymodels) # tidymodels framework ## ── Attaching packages ──────────────────── ## ✓ broom 0.7.0 ✓ recipes 0.1.13 ## ✓ dials 0.0.9 ✓ rsample 0.0.8 ## ✓ infer 0.5.3 ✓ tune 0.1.1 ## ✓ modeldata 0.0.2 ✓ workflows 0.2.0 ## ✓ parsnip 0.1.3 ✓ yardstick 0.0.7 ## ── Conflicts ─── tidymodels_conflicts() ── ## x scales::discard() masks purrr::discard() ## x dplyr::filter() masks stats::filter() ## x recipes::fixed() masks stringr::fixed() ## x dplyr::lag() masks stats::lag() ## x dials::prune() masks rpart::prune() ## x yardstick::spec() masks readr::spec() ## x recipes::step() masks stats::step() library(here) # reproducible way to find files ## here() starts at /home/jae/Machine-Learning-in-R library(glue) # glue strings and objects ## ## Attaching package: &#39;glue&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse library(patchwork) # arrange ggplots library(doParallel) # parallel processing ## Loading required package: foreach ## ## Attaching package: &#39;foreach&#39; ## The following objects are masked from &#39;package:purrr&#39;: ## ## accumulate, when ## Loading required package: iterators ## Loading required package: parallel source(here(&quot;functions&quot;, &quot;utils.R&quot;)) theme_set(theme_minimal()) 5.2 Load data Load train_x_class, train_y_class, test_x_class, and test_y_class variables we defined in 02-preprocessing.Rmd for this classification task. # Objects: task_reg, task_class load(here(&quot;data&quot;, &quot;preprocessed.RData&quot;)) 5.3 Overview Decision trees are recursive partitioning methods that divide the predictor spaces into simpler regions and can be visualized in a tree-like structure. They attempt to classify data by dividing it into subsets according to a Y output variable and based on some predictors. Let’s see how a decision tree classifies if a person suffers from heart disease (target = 1) or not (target = 0). 5.4 Non-tidy 5.4.1 Fit model set.seed(3) tree &lt;- rpart::rpart(train_y_class ~ ., data = train_x_class, # Use method = &quot;anova&quot; for a continuous outcome. method = &quot;class&quot;, # Can use &quot;gini&quot; for gini coefficient. parms = list(split = &quot;information&quot;)) # https://stackoverflow.com/questions/4553947/decision-tree-on-information-gain 5.4.2 Investigate Here is the text-based display of the decision tree. Yikes! :^( print(tree) ## n= 213 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 213 97 1 (0.45539906 0.54460094) ## 2) thal_X2&lt; 0.5 100 25 0 (0.75000000 0.25000000) ## 4) exang&gt;=0.5 47 3 0 (0.93617021 0.06382979) * ## 5) exang&lt; 0.5 53 22 0 (0.58490566 0.41509434) ## 10) thalach&lt; 130 7 0 0 (1.00000000 0.00000000) * ## 11) thalach&gt;=130 46 22 0 (0.52173913 0.47826087) ## 22) chol&gt;=244 19 5 0 (0.73684211 0.26315789) * ## 23) chol&lt; 244 27 10 1 (0.37037037 0.62962963) ## 46) chol&lt; 230 20 9 1 (0.45000000 0.55000000) ## 92) oldpeak&gt;=0.7 8 2 0 (0.75000000 0.25000000) * ## 93) oldpeak&lt; 0.7 12 3 1 (0.25000000 0.75000000) * ## 47) chol&gt;=230 7 1 1 (0.14285714 0.85714286) * ## 3) thal_X2&gt;=0.5 113 22 1 (0.19469027 0.80530973) ## 6) thalach&lt; 162.5 62 19 1 (0.30645161 0.69354839) ## 12) sex_X1&gt;=0.5 29 14 0 (0.51724138 0.48275862) ## 24) thalach&lt; 131.5 8 1 0 (0.87500000 0.12500000) * ## 25) thalach&gt;=131.5 21 8 1 (0.38095238 0.61904762) ## 50) thalach&gt;=156.5 7 2 0 (0.71428571 0.28571429) * ## 51) thalach&lt; 156.5 14 3 1 (0.21428571 0.78571429) * ## 13) sex_X1&lt; 0.5 33 4 1 (0.12121212 0.87878788) * ## 7) thalach&gt;=162.5 51 3 1 (0.05882353 0.94117647) * Although interpreting the text can be intimidating, a decision tree’s main strength is its tree-like plot, which is much easier to interpret. rpart.plot::rpart.plot(tree) We can also look inside of tree to see what we can unpack. “variable.importance” is one we should check out! names(tree) ## [1] &quot;frame&quot; &quot;where&quot; &quot;call&quot; ## [4] &quot;terms&quot; &quot;cptable&quot; &quot;method&quot; ## [7] &quot;parms&quot; &quot;control&quot; &quot;functions&quot; ## [10] &quot;numresp&quot; &quot;splits&quot; &quot;variable.importance&quot; ## [13] &quot;y&quot; &quot;ordered&quot; tree$variable.importance ## thal_X2 thalach thal_X3 exang oldpeak slope_X2 chol ## 34.8546153 33.5429632 29.6264230 20.1781088 19.8238634 12.8908263 9.4650464 ## sex_X1 age ca_X2 trestbps cp_X2 slope_X1 ca_X3 ## 6.4228665 3.4926165 2.1173837 1.9228895 0.9689174 0.8355620 0.5555638 5.5 Tidy models 5.5.1 parsnip Build a model Specify a model Specify an engine Specify a mode # workflow tree_wf &lt;- workflow() %&gt;% add_formula(target~.) # spec tree_spec &lt;- decision_tree( # Mode mode = &quot;classification&quot;, # Tuning parameters cost_complexity = NULL, tree_depth = NULL) %&gt;% set_engine(&quot;rpart&quot;) # rpart, c5.0, spark tree_wf &lt;- tree_wf %&gt;% add_model(tree_spec) Fit a model tree_fit &lt;- tree_wf %&gt;% fit(train_x_class %&gt;% bind_cols(tibble(target = train_y_class))) 5.5.2 yardstick Let’s formally test prediction performance. Metrics accuracy: The proportion of the data predicted correctly precision: Positive predictive value recall (specificity): True positive rate (e.g., healthy people really healthy) From wikipedia To learn more about other metrics, check out the yardstick package references. # Define performance metrics metrics &lt;- yardstick::metric_set(accuracy, precision, recall) # Visualize tree_fit_viz_metr &lt;- visualize_class_eval(tree_fit) tree_fit_viz_metr tree_fit_viz_mat &lt;- visualize_class_conf(tree_fit) tree_fit_viz_mat 5.5.3 tune 5.5.3.1 tune ingredients In decision trees the main hyperparameter (configuration setting) is the complexity parameter (CP), but the name is a little counterintuitive; a high CP results in a simple decision tree with few splits, whereas a low CP results in a larger decision tree with many splits. The other related hyperparameter is tree_depth. tune_spec &lt;- decision_tree( cost_complexity = tune(), tree_depth = tune(), mode = &quot;classification&quot; ) %&gt;% set_engine(&quot;rpart&quot;) tree_grid &lt;- grid_regular(cost_complexity(), tree_depth(), levels = 5) # 2 parameters -&gt; 5*5 = 25 combinations tree_grid %&gt;% count(tree_depth) ## # A tibble: 5 x 2 ## tree_depth n ## &lt;int&gt; &lt;int&gt; ## 1 1 5 ## 2 4 5 ## 3 8 5 ## 4 11 5 ## 5 15 5 # 10-fold cross-validation set.seed(1234) # for reproducibility tree_folds &lt;- vfold_cv(train_x_class %&gt;% bind_cols(tibble(target = train_y_class)), strata = target) 5.5.3.2 Add these elements to a workflow # Update workflow tree_wf &lt;- tree_wf %&gt;% update_model(tune_spec) cl &lt;- makeCluster(4) registerDoParallel(cl) # Tuning results tree_res &lt;- tree_wf %&gt;% tune_grid( resamples = tree_folds, grid = tree_grid, metrics = metrics ) 5.5.3.3 Visualize The following plot draws on the vignette of the tidymodels package. tree_res %&gt;% collect_metrics() %&gt;% mutate(tree_depth = factor(tree_depth)) %&gt;% ggplot(aes(cost_complexity, mean, col = .metric)) + geom_point(size = 3) + # Subplots facet_wrap(~ tree_depth, scales = &quot;free&quot;, nrow = 2) + # Log scale x scale_x_log10(labels = scales::label_number()) + # Discrete color scale scale_color_viridis_d(option = &quot;plasma&quot;, begin = .9, end = 0) + labs(x = &quot;Cost complexity&quot;, col = &quot;Tree depth&quot;, y = NULL) + coord_flip() # Optimal parameter best_tree &lt;- select_best(tree_res, &quot;recall&quot;) best_tree ## # A tibble: 1 x 3 ## cost_complexity tree_depth .config ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 0.0000000001 1 Model01 # Add the parameter to the workflow finalize_tree &lt;- tree_wf %&gt;% finalize_workflow(best_tree) tree_fit_tuned &lt;- finalize_tree %&gt;% fit(train_x_class %&gt;% bind_cols(tibble(target = train_y_class))) # Metrics (tree_fit_viz_metr + labs(title = &quot;Non-tuned&quot;)) / (visualize_class_eval(tree_fit_tuned) + labs(title = &quot;Tuned&quot;)) # Confusion matrix (tree_fit_viz_mat + labs(title = &quot;Non-tuned&quot;)) / (visualize_class_conf(tree_fit_tuned) + labs(title = &quot;Tuned&quot;)) Visualize variable importance tree_fit_tuned %&gt;% pull_workflow_fit() %&gt;% vip::vip() #### Test fit Apply the tuned model to the test dataset test_fit &lt;- finalize_tree %&gt;% fit(test_x_class %&gt;% bind_cols(tibble(target = test_y_class))) evaluate_class(test_fit) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.722 ## 2 precision binary 0.864 ## 3 recall binary 0.463 TBD: Challenge 2 "]
]
