[
["ols-and-lasso.html", "Chapter 4 OLS and lasso 4.1 Load packages 4.2 Load data 4.3 Overview 4.4 Fit and evaluate models 4.5 Tuning parameters 4.6 Challenge 1", " Chapter 4 OLS and lasso 4.1 Load packages library(glmnet) ## Loading required package: Matrix ## Loaded glmnet 4.0-2 library(rio) # painless data import and export library(tidyverse) # tidyverse packages ## ── Attaching packages ──────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.3 ✓ dplyr 1.0.2 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0 ## ── Conflicts ─────────────── tidyverse_conflicts() ── ## x tidyr::expand() masks Matrix::expand() ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() ## x tidyr::pack() masks Matrix::pack() ## x tidyr::unpack() masks Matrix::unpack() library(tidymodels) # tidymodels framework ## ── Attaching packages ─────────── tidymodels 0.1.1 ── ## ✓ broom 0.7.0 ✓ recipes 0.1.13 ## ✓ dials 0.0.9 ✓ rsample 0.0.7 ## ✓ infer 0.5.3 ✓ tune 0.1.1 ## ✓ modeldata 0.0.2 ✓ workflows 0.2.0 ## ✓ parsnip 0.1.3 ✓ yardstick 0.0.7 ## ── Conflicts ────────────── tidymodels_conflicts() ── ## x scales::discard() masks purrr::discard() ## x tidyr::expand() masks Matrix::expand() ## x dplyr::filter() masks stats::filter() ## x recipes::fixed() masks stringr::fixed() ## x dplyr::lag() masks stats::lag() ## x tidyr::pack() masks Matrix::pack() ## x yardstick::spec() masks readr::spec() ## x recipes::step() masks stats::step() ## x tidyr::unpack() masks Matrix::unpack() library(here) # reproducible way to find files ## here() starts at /home/jae/Machine-Learning-in-R library(glue) # glue strings and objects ## ## Attaching package: &#39;glue&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse theme_set(theme_minimal()) 4.2 Load data Load train_x_reg, train_y_reg, test_x_reg, and test_y_reg variables we defined in 02-preprocessing.Rmd for the OLS and Lasso regression tasks. # Objects: task_reg, task_class load(here(&quot;data&quot;, &quot;preprocessed.RData&quot;)) 4.3 Overview LASSO = sets Beta coefficients of unrelated (to Y) predictors to zero RIDGE = sets Beta coefficients of unrelated (to Y) predictors NEAR ZERO but does not remove them ELASTICNET = a combination of LASSO and RIDGE Review “Challenge 0” in the Challenges folder for a useful review of how OLS regression works and see the yhat blog for help interpreting its output. Linear regression is a useful introduction to machine learning, but in your research you might be faced with warning messages after predict() about the rank of your matrix. The lasso is useful to try and remove some of the non-associated features from the model. Because glmnet expects a matrix of predictors, use as.matrix to convert it from a data frame to a matrix. (You don’t need to worry about this, if you use tidymodels.) Be sure to read the glmnet vignette 4.4 Fit and evaluate models 4.4.1 Non-tidy 4.4.1.1 OLS Below is an refresher of ordinary least squares linear (OLS) regression that predicts age using the other variables as predictors. # Fit the regression model; lm() will automatically add a temporary intercept column ols &lt;- lm(train_y_reg ~ ., data = train_x_reg) # Predict outcome for the test data ols_predicted &lt;- predict(ols, test_x_reg) # Root mean-squared error sqrt(mean((test_y_reg - ols_predicted )^2)) ## [1] 8.074011 4.4.1.2 Lasso # Fit the lasso model lasso &lt;- cv.glmnet(x = as.matrix(train_x_reg), y = train_y_reg, family = &quot;gaussian&quot;, alpha = 1) lasso$lambda.min ## [1] 0.4194076 # Predict outcome for the test data lasso_predicted &lt;- predict(lasso, newx = as.matrix(test_x_reg), s = 0.1) # Tuning parameter # Calculate root mean-squared error sqrt(mean((lasso_predicted - test_y_reg)^2)) ## [1] 7.858063 4.4.2 tidymodels 4.4.2.1 parsnip Build models # OLS spec ols_spec &lt;- linear_reg() %&gt;% # Specify the model set_engine(&quot;lm&quot;) %&gt;% # Specify the engine: lm, glmnet, stan, keras, spark set_mode(&quot;regression&quot;) # Declare the mode: regression or classification # Lasso spec lasso_spec &lt;- linear_reg(penalty = 0.1, # tuning parameter mixture = 1) %&gt;% # 1 = lasso, 0 = ridge set_engine(&quot;glmnet&quot;) %&gt;% set_mode(&quot;regression&quot;) # If you don&#39;t understand parsnip arguments lasso_spec %&gt;% translate() # See the documentation ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = 0.1 ## mixture = 1 ## ## Computational engine: glmnet ## ## Model fit template: ## glmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), ## alpha = 1, family = &quot;gaussian&quot;) Fit models ols_fit &lt;- ols_spec %&gt;% fit_xy(x = train_x_reg, y= train_y_reg) # fit(train_y_reg ~ ., train_x_reg) # When you data are not preprocessed lasso_fit &lt;- lasso_spec %&gt;% fit_xy(x = train_x_reg, y= train_y_reg) 4.4.2.2 yardstick Evaluate prediction performance # Visualize model fit visualize_fit &lt;- function(model, names){ # Bind ground truth and predicted values bind_cols(tibble(truth = test_y_reg), # Ground truth predict(model, test_x_reg)) %&gt;% # Predicted values # Visualize the residuals ggplot(aes(x = truth, y = .pred)) + # Diagonal line geom_abline(lty = 2) + geom_point(alpha = 0.5) + # Make X- and Y- scale uniform coord_obs_pred() + labs(title = glue::glue(&quot;{names}&quot;)) } map2(list(ols_fit, lasso_fit), c(&quot;OLS&quot;, &quot;Lasso&quot;), visualize_fit) ## [[1]] ## ## [[2]] # Define performance metrics metrics &lt;- yardstick::metric_set(rmse, rsq, mae) # Build an evaluation function evaluate_model &lt;- function(model){ # Bind ground truth and predicted values bind_cols(tibble(truth = test_y_reg), # Ground truth predict(model, test_x_reg)) %&gt;% # Predicted values # Calculate root mean-squared error metrics(truth = truth, estimate = .pred) } # Evaluate many models evals &lt;- purrr::map(list(ols_fit, lasso_fit), evaluate_model) %&gt;% reduce(bind_rows) %&gt;% mutate(type = rep(c(&quot;OLS&quot;, &quot;Lasso&quot;), each = 3)) # Visualize the test results evals %&gt;% ggplot(aes(x = fct_reorder(type, .estimate), y = .estimate)) + geom_point() + labs(x = &quot;Model&quot;, y = &quot;Estimate&quot;) + facet_wrap(~glue(&quot;{toupper(.metric)}&quot;), scales = &quot;free_y&quot;) 4.5 Tuning parameters Visualize the distribution of log(lamba) vs mean-squared error. plot(lasso) # Help interpreting this plot: https://stats.stackexchange.com/questions/404795/interpretation-of-cross-validation-plot-for-lasso-regression # Generate our own version, but plot lambda (not on log scale) vs. RMSE. qplot(lasso$lambda, sqrt(lasso$cvm)) NOTE: when log(lamba) is equal to 0 that means lambda is equal to 1. In this graph, the far right side is overpenalized, as the model is emphasizing the beta coefficients being small. As log(lambda) becomes increasingly negative, lambda is correspondingly closer to zero and we are approaching the OLS solution. # And here is a plot of log(lambda) vs lambda. qplot(log(lasso$lambda), lasso$lambda) Show plot of different lambda values: plot(lasso$glmnet.fit, xvar = &quot;lambda&quot;, label = TRUE) Show the lambda that results in the minimum estimated mean-squared error (MSE): lasso$lambda.min ## [1] 0.4194076 Show higher lambda within one standard error of performance of the minimum lasso$lambda.1se ## [1] 1.167027 # Log scale versions: log(c(&quot;log_min&quot; = lasso$lambda.min, &quot;log_1se&quot; = lasso$lambda.1se)) ## log_min log_1se ## -0.8689121 0.1544591 Look at the coefficients (coef_1se = coef(lasso, s = &quot;lambda.1se&quot;)) ## 22 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 60.296179264 ## trestbps 0.071877196 ## chol 0.006372491 ## fbs . ## restecg . ## thalach -0.112828872 ## exang . ## oldpeak . ## target . ## sex_X1 . ## ca_X1 . ## ca_X2 1.669594563 ## ca_X3 . ## ca_X4 -0.634474556 ## cp_X1 . ## cp_X2 . ## cp_X3 . ## slope_X1 . ## slope_X2 . ## thal_X1 . ## thal_X2 . ## thal_X3 . Look at the coefficients for lambda.min (coef_min = coef(lasso, s = &quot;lambda.min&quot;)) ## 22 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 55.66199020 ## trestbps 0.11015680 ## chol 0.01309444 ## fbs . ## restecg -0.35221266 ## thalach -0.12678780 ## exang -0.17134774 ## oldpeak . ## target . ## sex_X1 -0.94868280 ## ca_X1 2.07593388 ## ca_X2 4.41461473 ## ca_X3 3.15152379 ## ca_X4 -4.41342359 ## cp_X1 . ## cp_X2 . ## cp_X3 . ## slope_X1 . ## slope_X2 . ## thal_X1 . ## thal_X2 . ## thal_X3 . # Compare side-by-side cbind(as.matrix(coef_1se), as.matrix(coef_min)) ## 1 1 ## (Intercept) 60.296179264 55.66199020 ## trestbps 0.071877196 0.11015680 ## chol 0.006372491 0.01309444 ## fbs 0.000000000 0.00000000 ## restecg 0.000000000 -0.35221266 ## thalach -0.112828872 -0.12678780 ## exang 0.000000000 -0.17134774 ## oldpeak 0.000000000 0.00000000 ## target 0.000000000 0.00000000 ## sex_X1 0.000000000 -0.94868280 ## ca_X1 0.000000000 2.07593388 ## ca_X2 1.669594563 4.41461473 ## ca_X3 0.000000000 3.15152379 ## ca_X4 -0.634474556 -4.41342359 ## cp_X1 0.000000000 0.00000000 ## cp_X2 0.000000000 0.00000000 ## cp_X3 0.000000000 0.00000000 ## slope_X1 0.000000000 0.00000000 ## slope_X2 0.000000000 0.00000000 ## thal_X1 0.000000000 0.00000000 ## thal_X2 0.000000000 0.00000000 ## thal_X3 0.000000000 0.00000000 Predict on the test set predictions &lt;- predict(lasso, newx = as.matrix(test_x_reg), s = lasso$lambda.1se) # How far off were we, based on absolute error? rounded_errors &lt;- round(abs(test_y_reg - predictions)) table(rounded_errors) ## rounded_errors ## 0 1 2 3 4 5 6 7 8 9 10 11 12 14 15 16 17 21 ## 4 9 5 7 7 5 8 9 9 1 7 5 3 4 1 4 1 1 # Group the absolute error into 4 bins. grouped_errors &lt;- round(abs(test_y_reg - predictions) / 5) grouped_errors[grouped_errors &gt; 2] = 3 table(grouped_errors) ## grouped_errors ## 0 1 2 3 ## 18 36 25 11 # 4 categories of accuracy how_close &lt;- factor(grouped_errors, labels = c(&quot;very close&quot;, &quot;close&quot;, &quot;meh&quot;, &quot;far&quot;)) table(rounded_errors, how_close) ## how_close ## rounded_errors very close close meh far ## 0 4 0 0 0 ## 1 9 0 0 0 ## 2 5 0 0 0 ## 3 0 7 0 0 ## 4 0 7 0 0 ## 5 0 5 0 0 ## 6 0 8 0 0 ## 7 0 9 0 0 ## 8 0 0 9 0 ## 9 0 0 1 0 ## 10 0 0 7 0 ## 11 0 0 5 0 ## 12 0 0 3 0 ## 14 0 0 0 4 ## 15 0 0 0 1 ## 16 0 0 0 4 ## 17 0 0 0 1 ## 21 0 0 0 1 # Scatter plot of actual vs. predicted qplot(test_y_reg, predictions, color = how_close) + geom_point(size = 5, alpha = 0.7) Calculate MSE and RMSE: # Calculate mean-squared error. mean((predictions - test_y_reg)^2) ## [1] 68.94235 # Calculate root mean-squared error. sqrt(mean((predictions - test_y_reg)^2)) ## [1] 8.303153 4.6 Challenge 1 Open Challenge 1 in the “Challenges” folder. "]
]
