
# Decision Trees

## Load packages

```{r load_packages}

library(rpart)
library(rpart.plot)
library(rio) # painless data import and export
library(tidyverse) # tidyverse packages 
library(tidymodels) # tidymodels framework 
library(here) # reproducible way to find files 
library(glue) # glue strings and objects 

theme_set(theme_minimal())

```

## Load data 

Load `train_x_class`, `train_y_class`, `test_x_class`, and `test_y_class` variables we defined in 02-preprocessing.Rmd for this *classification* task. 

```{r setup_data}
# Objects: task_reg, task_class
load(here("data", "preprocessed.RData"))
```

## Overview

Decision trees are recursive partitioning methods that divide the predictor spaces into simpler regions and can be visualized in a tree-like structure. They attempt to classify data by dividing it into subsets according to a Y output variable and based on some predictors.  

Let's see how a decision tree classifies if a person suffers from heart disease (`target` = 1) or not (`target` = 0).

## Non-tidy

### Fit a model 

```{r}

set.seed(3)

tree <- rpart::rpart(train_y_class ~ ., data = train_x_class,
             # Use method = "anova" for a continuous outcome.
             method = "class",
             
             # Can use "gini" for gini coefficient.
             parms = list(split = "information")) 

# https://stackoverflow.com/questions/4553947/decision-tree-on-information-gain

```

### Investigate 

- Here is the text-based display of the decision tree. Yikes!  :^( 

```{r}

print(tree)

```

Although interpreting the text can be intimidating, a decision tree's main strength is its tree-like plot, which is much easier to interpret.

```{r plot_tree}
rpart.plot::rpart.plot(tree) 
```

We can also look inside of `tree` to see what we can unpack. "variable.importance" is one we should check out! 

```{r}

names(tree)

tree$variable.importance

```
## Tidy models 

### parsnip 

- Build a model 

1. Specify a model 
2. Specify an engine 
3. Specify a mode 

```{r}

tree_spec <- decision_tree(
  
           # Mode 
           mode = "classification",
           
           # Tuning parameters; Not optimized yet  
           cost_complexity = NULL) %>%
  set_engine("rpart") # rpart, c5.0, spark 
  
```

- Fit a model

```{r}

tree_fit <- tree_spec %>%
  fit_xy(x = train_x_class, y= train_y_class)

```

### yardstick 

- Let's formally test prediction performance. 

**Metrics**

- `accuracy`: The proportion of the data predicted correctly 

- `precision`: Positive predictive value

- `recall` (specificity): True positive rate (e.g., healthy people really healthy)

![From wikipedia](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/525px-Precisionrecall.svg.png)

- To learn more about other metrics, check out the yardstick package [references](https://yardstick.tidymodels.org/reference/index.html). 

```{r}

# Define performance metrics 
metrics <- yardstick::metric_set(accuracy, precision, recall)

# Build an evaluation function 
evaluate_class <- function(model){
  
  # Bind ground truth and predicted values  
  bind_cols(tibble(truth = test_y_class), # Ground truth 
            predict(model, test_x_class)) %>% # Predicted values 
    
  # Calculate root mean-squared error
  metrics(truth = truth, estimate = .pred_class) 
}

```

```{r}

visualize_metrics <- function(model){
evaluate_class(model) %>%
  ggplot(aes(x = fct_reorder(.metric, .estimate), y = .estimate)) +
    geom_point() +
    labs(x = "Metrics",
         y = "Estimate")}

visualize_metrics(tree_fit)

```
- Visualize the confusion matrix. 

  - The following visualization code draws on [Diego Usai's medium post](https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c).
  
```{r}

visualize_conf_mat <- function(model){
  bind_cols(tibble(truth = test_y_class), # Ground truth 
            predict(model, test_x_class)) %>%
  conf_mat(truth, .pred_class) %>%
  pluck(1) %>% # Select index 
  as_tibble() %>% # Vector -> data.frame 
  ggplot(aes(Prediction, Truth, alpha = n)) +
    geom_tile(show.legend = FALSE) +
    geom_text(aes(label = n), 
              color = "red", 
              alpha = 1, 
              size = 13)
}

visualize_conf_mat(tree_fit)

```

### tune 

In decision trees the main hyperparameter (configuration setting) is the **complexity parameter** (CP), but the name is a little counterintuitive; a high CP results in a simple decision tree with few splits, whereas a low CP results in a larger decision tree with many splits.  

`rpart` uses cross-validation internally to estimate the accuracy at various CP settings. We can review those to see what setting seems best.  

Print the results for various CP settings - we want the one with the lowest "xerror". We can also plot the performance estimates for different CP settings. 

Trees of similar sizes might appear to be tied for lowest "xerror", but a tree with fewer splits might be easier to interpret. 

```{r plotcp_tree}
# Show estimated error rate at different complexity parameter settings.
printcp(tree)

# Plot those estimated error rates.
plotcp(tree)

```

Print detailed results, variable importance, and summary of splits.

You can also get more fine-grained control by checking out the "control" argument inside the rpart function. Type `?rpart` to learn more.  

Be sure to check out [gormanalysis](https://www.gormanalysis.com/blog/decision-trees-in-r-using-rpart/) excellent overview to help internalize what you learned in this example. 